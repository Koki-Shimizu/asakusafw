=========================
Direct I/O ユーザーガイド
=========================

この文書では、Direct I/Oの利用方法について紹介します。
Direct I/Oの導入方法については :doc:`../administration/deployment-with-directio` を参照してください。

..  caution::
    Direct I/O はAsakusa Frameworkのバージョン ``0.2.5`` において実験的な機能として提供しています。
    今後のバージョンで利用方法や挙動の一部が変更される可能性があります。


データソース
============
Direct I/Oを利用したバッチアプリケーションでは、ジョブフローの入出力データをHadoopから直接読み書きします。
このとき、読み書きする対象のことを「データソース」として抽象化しています。

それぞれのデータソースはいずれかの「論理パス」上に配置します。
このパスは唯一のルートを持つツリー状の構造で、ルートを含む任意のノードに対してデータソースを配置できます。
これらの配置設定はAsakusa Framework側の設定ファイル上で行います。

DSLではデータソースそのものを指定せずに、入出力を行う先の論理パスのみを指定します。
ジョブフローの実行時には論理パスからデータソースを引き当て、そのデータソースに対する入出力が行われます。


論理パスの解決
--------------
DSLで指定した論理パスから実行時にデータソースを引き当てる際、次のような方法でデータソースの検索が行われます。

#. 論理パスに対してデータソースが配置されている場合、そのデータソースを利用する
#. 論理パスに対してデータソースが配置されていない場合、現在の論理パスの親パスに対して再帰的にデータソースの検索を行う
#. ただし、現在の論理パスがルートである場合、データソースの検索は失敗する

つまり、DSLで指定した論理パスに対して、親方向に最も近いデータソースを検索して利用しています。

また、データソースを配置した論理パスよりもDSLで指定した論理パスの方が長い (つまり、サブパスが指定された) 場合、
データソースを配置した論理パスからの相対パスをファイルパスの先頭に利用します。

たとえば、データソースを ``a/b`` に配置し、DSLでは論理パスに ``a/b/c/d`` と指定した場合、データソースからの相対パスは ``c/d`` となります。さらにDSLでファイルパスに ``e/f`` と指定すると、結果のファイルパスは ``c/d/e/f`` となります。

..  note::
    この論理パスの機構は、Unixのファイルシステムのマウントを参考に設計しています。


入力のアーキテクチャ
====================
Direct I/Oの入力には次のような特徴があります。

* 大きなデータを分割して分散して読みだす

..  warning::
    バージョン ``0.2.5`` では、同一の入力を複数回読む場合があります。
    これは将来のバージョンで改善される予定です。

以降では上記の特徴に関するアーキテクチャを紹介します。

入力データの分割
----------------
Direct I/Oでデータを読み出す際、以下の条件をすべて満たした場合にデータを小さな断片に分割し、断片ごとに分散して読み出します。

* データストアが分割をサポートしている
* データの形式が分割をサポートしている
* データが十分に大きい

基本的にこの機能はHadoopの「入力スプリット」の仕組みを利用して実現しています。
ただし、データストアごとに細かく分割の単位を指定できるなどの相違があります。

分散キャッシュと連携する場合の動作
----------------------------------
Asakusa Frameworkでは入力データをHadoopの分散キャッシュに乗せて、処理を高速化するような最適化が行われる場合があります。
そのような場合、入力データはHadoopのファイルシステム上に一度だけコピーしたのち、コピーした内容をスレーブノードに配布しています。


出力のアーキテクチャ
====================
Direct I/Oの出力には次のような特徴があります。

* 出力ファイルを内容によって分割したりソートしたりできる
* Direct I/O全体の出力を簡易的なトランザクションで行う

以降では上記の特徴に関するアーキテクチャを紹介します。

出力ファイルの分割と内容のソート
--------------------------------
Direct I/Oでファイルを出力する際、出力内容を元にファイルの分割と内容のソートを行っています。
これは現在、Map Reduceの仕組みの上で以下のように行っています。

* Mapperで出力内容のファイル名、順序計算のためのプロパティからシャッフル用のキーを生成する
* シャッフル時にファイル名でグルーピングし、さらに順序計算のプロパティをもとにソートする (セカンダリソート)
* Reduceではファイル名ごとにグループができるため、グループごとにファイルを一つ出力する

このため、出力ファイルが1つである場合や、大きさに極端な偏りが見られる場合には全体の性能が低下します。

簡易的な出力のトランザクション
------------------------------
Direct I/Oで複数のデータソースに対して出力を行う際、内部では簡易的なトランザクション処理を行っています。
これにより、いずれかのデータソースに出力する際にエラーとなってしまった場合でも、最終的な出力先のデータを全く破壊しないか、
または整合性のとれた出力結果を得られるようになっています。

..  warning::
    バージョン ``0.2.5`` ではトランザクションの修復を自動的には行いません。
    `トランザクションのメンテナンス`_ を参考に、手動で修復を行ってください。

Direct I/Oのトランザクション処理は主に以下の流れで行います。

1. システムディレクトリ [#]_ に「トランザクション情報ファイル」を作成する
2. それぞれのMap Reduceのタスク試行で、出力をデータソースの「試行領域 (attempt area)」に書き出す
3. タスク試行が成功した場合、それぞれの試行領域のデータをデータソースの「ステージング領域 (staging area)」に移動する

  * 失敗した場合は試行領域のデータをクリアする

4. すべてのタスクが成功した場合、システムディレクトリに「コミットマークファイル」をアトミックに作成する

  * いずれかが失敗した場合はコミットマークファイルを作成しない

5. コミットマークファイルが存在する場合、それぞれのステージング領域の内容を最終的な出力先に移動し、コミットマークファイルを削除する

  * コミットマークファイルが存在しない場合は、ステージング領域の内容をクリアする

6. トランザクション情報ファイルを削除する

なお、試行領域はタスク試行ごとに、ステージング領域はデータソースごとにそれぞれ作成されます。

..  attention::
    標準的なデータソースでは、すでに移動先にデータが存在する場合に上書きします。

上記の仕組み上、Direct I/Oによる出力には次のような制約があります。

* 試行領域 > ステージング領域 > 最終的な出力先 とデータを移動させるため、データの移動に時間がかかるデータソースでは速度が出ない
* コミットマークファイル作成から削除までの間、データソースは一時的に整合性が失われる
* コミットマークファイル作成から削除までの間に処理が失敗した場合、修復が行われるまで整合性が失われる [#]_
* コミットマークファイル自体が障害によって失われた場合、データソースの整合性が失われる
* 同一の出力先に対して複数のジョブフローから出力を行った場合、結果が不安定になる (競合に対するロック等は行わない)

..  [#] 設定方法については `システムディレクトリの設定`_ を参照してください。
..  [#] 修復手順は `トランザクションのメンテナンス`_ を参照してください。


データソースの設定
==================
Direct I/Oの機構を利用するには、入出力の仲介を行う「データソース」の設定が必要です。

これらの設定は、 ``$ASAKUSA_HOME`` で指定したディレクトリ以下の ``core/conf/asakusa-resources.xml`` (以下、設定ファイル)内に、Hadoopの設定ファイルと同様の形式でそれぞれ記述していきます。

..  attention::
    現在のところ、設定ファイル内にシステムプロパティ ( ``${...}`` ) を指定できません。

データソースの追加
------------------
データソースを追加するには設定ファイルに次の項目を追加します。

..  list-table:: データソースを追加する際の設定
    :widths: 30 30
    :header-rows: 1

    * - 名前
      - 値
    * - ``com.asakusafw.directio.<DSID>``
      - データソースの実装クラス名
    * - ``com.asakusafw.directio.<DSID>.path``
      - データソースを配置する論理パス

設定の名前に含まれる ``<DSID>`` はそれぞれのデータソースを表す識別子です。
この識別子には半角アルファベットの大文字小文字、半角数字、半角アンダースコア ( ``_`` ) の組み合わせを指定できます。
複数のデータソースを利用する場合にはデータソースごとに識別子を変えて指定してください。

データソースの実装は、現在のところ `Hadoopのファイルシステムを利用したデータソース`_ のみを提供しています。
詳しくは対象の項を参照してください。

論理パスとはDirect I/Oのそれぞれのデータソースを配置する仮想的なパスで、DSLからこのパスを指定してデータソースを利用します。
このパスは ``alpha/beta/gamma`` のように名前をスラッシュ ( ``/`` ) で区切って書きます。
特別なパスとして、ルートパスは ``/`` 一文字で指定します。


Hadoopのファイルシステムを利用したデータソース
----------------------------------------------
データソースの実装として、HadoopのファイルシステムAPI ( ``FileSystem`` [#]_ ) を利用したものを提供しています。

本データソースを利用する場合、実装クラス名 ( ``com.asakusafw.directio.<DSID>`` ) には ``com.asakusafw.runtime.directio.hadoop.HadoopDataSource`` を指定します。
また、利用するファイルシステムについては、Hadoopの本体側であらかじめ設定を行っておく必要があります。

Direct I/Oの設定ファイルには、対象のデータソースに対してさらに以下の設定が必要です。

..  list-table:: Hadoopのファイルシステムを利用したデータソース
    :widths: 30 5 25
    :header-rows: 1

    * - 名前
      - 形式
      - 値
    * - ``com.asakusafw.directio.<DSID>.fs.path``
      - URI
      - ファイルシステム上のパス

ファイルシステム上のパスには次の3種類の形式を指定できます。

相対パス
    Hadoopのデフォルトファイルシステム [#]_ のワーキングディレクトリからの相対パスを利用します。
    
    なお、デフォルトファイルシステムにローカルファイルシステムを指定している場合、
    ワーキングディレクトリは必ずユーザーのホームディレクトリになります。

絶対パス
    Hadoopのデフォルトファイルシステム上の絶対パスを利用します。
    
    たとえば ``/var/log`` や ``/tmp/directio`` などです。

完全URI
    URIに対応するファイルシステム、ホスト、パスを利用します。
    
    たとえば ``file:///home/asakusa`` や ``hdfs://localhost:8020/user/asakusa`` などです。

..  warning::
    ファイルシステムパス以下はテスト実行時に全て削除されます。
    特にスタンドアロンモードのHadoopを利用時に相対パスを指定した場合、
    ホームディレクトリを起点としたパスと解釈されるため注意が必要です。


..  [#] ``org.apache.hadoop.fs.FileSystem``
..  [#] ``$HADOOP_HOME/conf/core-site.xml`` 内の ``fs.default.name`` に指定したファイルシステムです。


論理パスとファイルシステムパスの対応付け
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Hadoopのファイルシステムを利用したデータソースでは、指定したファイルシステム上のパス ( ``com.asakusafw.directio.<DSID>.fs.path`` ) を起点に論理パスとファイルを対応付けます。具体的には、次のような手順で対応付けます。

* DSLで指定した論理パスとファイル名から、 `論理パスの解決`_ にある方法で実際のファイルパスを計算する
* 計算したファイルパスを、指定したファイルシステム上のパスからの相対パスとみなす

たとえば、データソースを ``hadoop`` に配置し、DSLでは論理パスに ``hadoop/asakusa`` , ファイルパスに ``data.csv`` と指定した場合、実際に利用するファイルパスは ``asakusa/data.csv`` となります。さらに起点となるファイルシステム上のパスが ``hdfs://localhost/user`` であった場合、対応付けられる最終的なファイルシステム上のパスは ``hdfs://localhost/user/asakusa/data.csv`` となります。


ファイルの分割読み出しの設定
~~~~~~~~~~~~~~~~~~~~~~~~~~~~
`Hadoopのファイルシステムを利用したデータソース`_ において、 `入力データの分割`_ は次のように設定します。
いずれのプロパティも必須ではありません。

..  list-table:: ファイルの分割読み出しに関する設定
    :widths: 30 5 20
    :header-rows: 1
    
    * - 名前
      - 形式
      - 値
    * - ``com.asakusafw.directio.<DSID>.fragment.min``
      - long
      - 断片の最小バイト数
    * - ``com.asakusafw.directio.<DSID>.fragment.pref``
      - long
      - 断片の推奨バイト数

``...fragment.min`` に0未満の値を指定した場合、入力データの分割は行われません。
未指定の場合は 16MB 程度に設定されます。

``...fragment.pref`` が未指定の場合、 64MB程度に設定されます。
また、 ``...fragment.min`` 未満の値は設定できません。

分割の最小バイト数や推奨バイト数はデータの形式で上書きされることがあります。

* データの形式が入力データの分割を許可しない場合、ファイルは分割されない
* データの形式で指定した最小バイト数がデータソースで指定したものより大きな場合、データの形式で指定したものを優先する
* データの形式で推奨バイト数が指定されている場合、データの形式で指定したものを優先する
* 推奨バイト数が最小バイト数未満になる場合、推奨バイト数は最小バイト数の値を利用する

入力データの分割を許可している場合、このデータソースにおいてそれぞれの断片は次の制約をすべて満たします。

* それぞれの断片は最小バイト数未満にならない
* それぞれの断片は推奨バイト数の2倍以上にならない

..  note::
    Hadoop本体に指定したスプリットの設定はここでは使用しません。
    通常の場合は既定の設定値で問題なく動作するはずですが、
    ファイルの途中からデータを読み出すような操作に多大なコストがかかるようなファイルシステムにおいては、
    ファイルの分割を行わないなどの設定が必要になります。


トランザクションの設定
~~~~~~~~~~~~~~~~~~~~~~
`Hadoopのファイルシステムを利用したデータソース`_ において、 `簡易的な出力のトランザクション`_ は次のように設定します。
いずれのプロパティも必須ではありません。

..  list-table:: トランザクションに関する設定
    :widths: 25 5 30
    :header-rows: 1
    
    * - 名前
      - 形式
      - 値
    * - ``com.asakusafw.directio.<DSID>.fs.tempdir``
      - URI
      - トランザクション用に利用するファイルシステム上のパス
    * - ``com.asakusafw.directio.<DSID>.output.staging``
      - boolean
      - ステージング領域を利用するかどうか
    * - ``com.asakusafw.directio.<DSID>.output.streaming``
      - boolean
      - 試行領域に直接出力するかどうか

``...fs.tempdir`` を省略した場合、このパスは ``com.asakusafw.directio.<DSID>.fs.path`` 下の ``_directio_temp`` というディレクトリになります。
明示的に設定を行う場合、この値は  ``...fs.path`` と同一のファイルシステムでなければなりません [#]_ 。

``...output.staging`` を省略した場合、この値は ``true`` (ステージング領域を利用する) となります。
ステージング領域を利用しない場合、タスク試行の完了時に最終的な出力先へ結果のデータを直接移動します。

``...output.streaming`` を省略した場合、この値は ``true`` (試行領域に直接出力する) となります。
試行領域に直接出力しない場合、ローカルテンポラリ領域にファイルを作成したのち、タスク試行成功時にステージング領域にファイルを移動します。
この時利用するローカルテンポラリ領域は `ローカルテンポラリ領域の設定`_ があらかじめ必要です [#]_ 。


トランザクションが修復不可能な状態になった場合や、タスク試行中にHadoopそのものが異常終了した場合、 ``...fs.tempdir`` 以下に処理の途中結果が残されている場合があります。

..  [#] 具体的には、 ``...fs.tempdir`` 以下のファイルを ``...fs.path`` 以下のディレクトリに ``FileSystem.rename()`` で移動できる必要があります。
..  [#] 試行領域に直接出力をしない場合にローカルテンポラリ領域が設定されていないと実行時にエラーとなります。

HDFSでの設定例
~~~~~~~~~~~~~~
以下はHDFSの入出力を行う場合の設定例です。

..  code-block:: xml

    <property>
        <name>com.asakusafw.directio.hdfs</name>
        <value>com.asakusafw.runtime.directio.hadoop.HadoopDataSource</value>
    </property>
    <property>
        <name>com.asakusafw.directio.hdfs.path</name>
        <value>hdfs/var</value>
    </property>
    <property>
        <name>com.asakusafw.directio.hdfs.fs.path</name>
        <value>hdfs://localhost:8020/var/asakusa</value>
    </property>

HDFSは直接の出力やファイルの移動を低コストで行えるようになっています。
そのため、特別な設定を行わなくてもそれなりに動作します。

Amazon S3での設定例
~~~~~~~~~~~~~~~~~~~
Amazon Simple Storage Service ( `Amazon S3`_ )の入出力を行う場合の設定例です。

..  code-block:: xml

    <property>
        <name>com.asakusafw.directio.s3</name>
        <value>com.asakusafw.runtime.directio.hadoop.HadoopDataSource</value>
    </property>
    <property>
        <name>com.asakusafw.directio.s3.path</name>
        <value>s3/spool</value>
    </property>
    <property>
        <name>com.asakusafw.directio.s3.fs.path</name>
        <value>s3://example/var/spool</value>
    </property>
    <property>
        <name>com.asakusafw.directio.s3.fragment.min</name>
        <value>-1</value>
    </property>
    <property>
        <name>com.asakusafw.directio.s3.output.staging</name>
        <value>false</value>
    </property>
    <property>
        <name>com.asakusafw.directio.s3.output.streaming</name>
        <value>false</value>
    </property>
    <property>
        <name>com.asakusafw.output.local.tempdir</name>
        <value>/tmp/asakusa-directio</value>
    </property>

2012年1月現在、Hadoopのファイルシステムを経由してS3を利用する場合、入力データの分割や出力ファイルの移動にコストがかかるようです。
このため、上記の設定では主に次のようなことを行っています。

* 入力データの分割を行わない ( ``...fragment.min = -1`` )
* 試行領域をローカルファイルシステム上に作成する ( ``...output.streaming = false`` )
* ステージ領域をスキップする ( ``...output.staging = false`` )

..  _`Amazon S3`: http://aws.amazon.com/s3/


その他の設定
============
データソースの設定以外に、Direct I/Oの全体を通した設定を行えます。

システムディレクトリの設定
--------------------------
システムディレクトリはDirect I/Oの管理情報を保持するためのディレクトリで、以下の形式で設定します。
この内容はHadoop本体の設定ファイルに書いても、Direct I/Oの設定ファイルに書いてもどちらでも有効です [#]_ 。

..  list-table:: システムディレクトリの設定
    :widths: 20 5 30
    :header-rows: 1
    
    * - 名前
      - 形式
      - 値
    * - ``com.asakusafw.output.system.dir``
      - URI
      - Hadoopファイルシステム上のシステムディレクトリ

システムディレクトリの設定が省略された場合、Hadoopが利用するデフォルトファイルシステム上の、 ``<ワーキングディレクトリ>/_directio`` を利用します。
またプロパティの値の中に、Javaのシステムプロパティを ``${システムプロパティ名}`` という形式で利用できます。

..  note::
    システムディレクトリはトランザクションの管理情報など、Direct I/Oを利用するうえで重要な情報が記録されます。
    そのため、信頼性の高いデータストア上か、Direct I/Oを利用するうえで重要性の高いデータストアと同じ領域内に配置することを推奨します。

..  [#] 正確に言えば、データソースの設定もHadoop本体の設定ファイル内に記載できます。
    ただし、データソースの設定はDirect I/O独自の設定ファイルに記載することを推奨します。

ローカルテンポラリ領域の設定
----------------------------
ローカルテンポラリ領域は、Direct I/Oが利用するHadoopスレーブノードのローカルファイルシステム上のディレクトリです。
タスク試行の実行中に一時的に利用します。


この内容は以下の形式で設定します。
なお、Hadoop本体の設定ファイルに書いても、Direct I/Oの設定ファイルに書いてもどちらでも有効です。

..  list-table:: ローカルテンポラリ領域の設定
    :widths: 20 10 30
    :header-rows: 1
    
    * - 名前
      - 形式
      - 値
    * - ``com.asakusafw.output.local.tempdir``
      - ファイルパス
      - ローカルファイルシステム上のテンポラリディレクトリ

ローカルテンポラリ領域はローカルファイルシステム上の絶対パスを指定します。
この設定が省略された場合、ローカルテンポラリ領域は利用できなくなります。

設定に対するディレクトリが存在しない場合、ローカルテンポラリ領域の利用時に自動的にディレクトリを作成します。


ログの設定
----------
Direct I/Oに関するログはHadoop本体のログの設定を利用して行います。
Hadoop本体の関連するドキュメントを参照してください。


ファイルの入出力
================
Direct I/Oを利用してファイルを入出力するには、 `Hadoopのファイルシステムを利用したデータソース`_ などの設定をしておきます。

また、データモデルと対象のファイル形式をマッピングする ``BinaryStreamFormat`` [#]_ の作成が必要です。
この実装クラスは、DMDLコンパイラの拡張 [#]_ を利用して自動的に生成できます。

なお、以降の機能を利用するには次のライブラリやプラグインが必要です。

..  list-table:: Direct I/Oで利用するライブラリ等
    :widths: 50 50
    :header-rows: 1

    * - ライブラリ
      - 概要
    * - ``asakusa-directio-vocabulary``
      - DSL用のクラス群
    * - ``asakusa-directio-plugin``
      - DSLコンパイラプラグイン
    * - ``asakusa-directio-test-moderator``
      - テストドライバプラグイン
    * - ``asakusa-directio-dmdl``
      - DMDLコンパイラプラグイン

..  [#] ``com.asakusafw.runtime.directio.BinaryStreamFormat``
..  [#] :doc:`../dmdl/user-guide` を参照

CSV形式のBinaryStreamFormatの作成
---------------------------------
CSV形式 [#]_ に対応した ``BinaryStreamFormat`` の実装クラスを自動的に生成するには、対象のデータモデルに ``@directio.csv`` を指定します。

..  code-block:: none

    @directio.csv
    document = {
        "the name of this document"
        name : TEXT;

        "the content of this document"
        content : TEXT;
    };

上記のように記述してデータモデルクラスを生成すると、 ``<出力先パッケージ>.csv.<データモデル名>CsvFormat`` というクラスが自動生成されます。
このクラスは ``BinaryStreamFormat`` を実装し、データモデル内のプロパティが順番に並んでいるCSVを取り扱えます。

また、 単純な `ファイルを入力に利用するDSL`_ と `ファイルを出力に利用するDSL`_ の骨格も自動生成します。前者は ``<出力先パッケージ>.csv.Abstract<データモデル名>CsvInputDescription`` 、後者は ``<出力先パッケージ>.csv.Abstract<データモデル名>CsvOutputDescription`` というクラス名で生成します。必要に応じて継承して利用してください。

この機能を利用するには、DMDLコンパイラのプラグインに ``asakusa-directio-dmdl`` を追加する必要があります。
DMDLコンパイラについては :doc:`../dmdl/user-guide` を参照してください。

..  note::
    この機構は :doc:`WindGate <../windgate/user-guide>` のものと将来統合されるかもしれません。

..  [#] ここでのCSV形式は、RFC 4180 (http://www.ietf.org/rfc/rfc4180.txt) で提唱されている形式を一部変更したものです。
    文字セットをASCIIの範囲外にも拡張したり、CRLF以外にもLFのみも改行と見なしたり、ダブルクウォート文字の取り扱いを緩くしたりなどの拡張を加えています。
    `CSV形式の注意点`_ も参照してください。


CSV形式の設定
~~~~~~~~~~~~~
``@directio.csv`` 属性には、次のような要素を指定できます。

..  list-table:: CSV形式の設定
    :widths: 10 10 20 60
    :header-rows: 1

    * - 要素
      - 型
      - 既定値
      - 内容
    * - ``charset``
      - 文字列
      - ``"UTF-8"``
      - ファイルの文字エンコーディング
    * - ``allow_linefeed``
      - 論理値
      - ``FALSE``
      - ``TRUE`` で値内にLFを含められる。 ``FALSE`` で不許可
    * - ``has_header``
      - 論理値
      - ``FALSE``
      - ``TRUE`` でヘッダの利用を許可。 ``FALSE`` で不許可
    * - ``true``
      - 文字列
      - ``"true"``
      - ``BOOLEAN`` 型の ``TRUE`` 値の表現形式
    * - ``false``
      - 文字列
      - ``"false"``
      - ``BOOLEAN`` 型の ``FALSE`` 値の表現形式
    * - ``date``
      - 文字列
      - ``"yyyy-MM-dd"``
      - ``DATE`` 型の表現形式
    * - ``datetime``
      - 文字列
      - ``"yyyy-MM-dd HH:mm:ss"``
      - ``DATETIME`` 型の表現形式

なお、 ``date`` および ``datetime`` には ``SimpleDateFormat`` [#]_ の形式で日付や時刻を指定します。

..  attention::
    デフォルトでは ``allow_linefeed`` には ``FALSE`` が設定されていて、文字列の内部などに改行文字 LF を含められないようになっています。
    この設定を ``TRUE`` にすることでLFを含められるようになりますが、代わりに `入力データの分割`_ が行われなくなります。
    詳しくは `CSV形式の注意点`_ を参照してください。

以下は記述例です。

..  code-block:: none

    @directio.csv(
        charset = "ISO-2022-JP",
        allow_linefeed = TRUE,
        has_header = TRUE,
        true = "1",
        false = "0",
        date = "yyyy/MM/dd",
        datetime = "yyyy/MM/dd HH:mm:ss",
    )
    model = {
        ...
    };

..  [#] ``java.text.SimpleDateFormat``


ヘッダの設定
~~~~~~~~~~~~
`CSV形式の設定`_ でヘッダを有効にしている場合、出力の一行目にプロパティ名が表示されます。
ここで表示される内容を変更するには、それぞれのプロパティに ``@directio.csv.field`` 属性を指定し、さらに ``name`` 要素でフィールド名を指定します。

以下は利用例です。

..  code-block:: none

    @directio.csv
    document = {
        "the name of this document"
        @directio.csv.field(name = "題名")
        name : TEXT;

        "the content of this document"
        @directio.csv.field(name = "内容")
        content : TEXT;
    };

ファイル情報の取得
~~~~~~~~~~~~~~~~~~
解析中のCSVファイルに関する属性を取得する場合、それぞれ以下の属性をプロパティに指定します。

..  list-table:: ファイル情報の取得に関する属性
    :widths: 30 10 20
    :header-rows: 1

    * - 属性
      - 型
      - 内容
    * - ``@directio.csv.file_name``
      - ``TEXT``
      - ファイル名
    * - ``@directio.csv.line_number``
      - ``INT`` , ``LONG``
      - テキスト行番号 (1起算)
    * - ``@directio.csv.record_number``
      - ``INT`` , ``LONG``
      - レコード番号 (1起算)

上記の属性が指定されたプロパティは、CSVのフィールドから除外されます。

..  attention::
    ``@directio.csv.line_number`` または ``@directio.csv.record_number`` が指定された場合、 `入力データの分割`_ が行われなくなります。
    詳しくは `CSV形式の注意点`_ を参照してください。

..  attention::
    これらの属性はCSVの解析時のみ有効です。
    CSVを書き出す際には無視されます。

CSVから除外するプロパティ
~~~~~~~~~~~~~~~~~~~~~~~~~
特定のプロパティをCSVのフィールドとして取り扱いたくない場合、プロパティに ``@directio.csv.ignore`` を指定します。

CSV形式の注意点
~~~~~~~~~~~~~~~
自動生成でサポートするCSV形式を利用するうえで、いくつかの注意点があります。

* 改行文字は CRLF または LF のみ、CRのみです

  * ただしCRのみを利用している場合、入力データの分割が正しく行われません

* CSVに空の文字列を書き出しても、読み出し時に ``null`` として取り扱われます
* 論理値は復元時に、値が ``true`` で指定した文字列の場合には ``true`` , 空の場合には ``null`` , それ以外の場合には ``false`` となります
* ヘッダが一文字でも異なる場合、解析時にヘッダとして取り扱われません
* 1レコードが10MBを超える場合、正しく解析できません
* 以下のいずれかが指定された場合、 `入力データの分割`_ は行われなくなります

  * ``@directio.csv( allow_linefeed = TRUE )``
  * ``@directio.csv.line_number``
  * ``@directio.csv.record_number``


ファイルを入力に利用するDSL
---------------------------
Direct I/Oを利用してファイルからデータを読み出す場合、 ``DirectFileInputDescription`` [#]_ クラスのサブクラスを作成して必要な情報を記述します。

このクラスでは、下記のメソッドをオーバーライドします。

``String getBasePath()``
    入力に利用する論理パスを戻り値に指定します。

    ここには ``${変数名}`` の形式で、バッチ起動時の引数やあらかじめ宣言された変数を利用できます。
    利用可能な変数はコンテキストAPIで参照できるものと同様です。

``String getResourcePattern()``
    入力に利用するファイル名のパターンを戻り値に指定します。
    ``getBasePath()`` で指定したパスを起点に、このパターンの名前を持つファイルを検索します。

    形式については `入力ファイル名のパターン`_ を参照してください。

``Class<?> getModelType()``
    処理対象とするモデルオブジェクトの型を表すクラスを戻り値に指定します。

    このメソッドは、自動生成される骨格ではすでに宣言されています。

``Class<? extends BinaryStreamFormat<?>> getFormat()``
    ``BinaryStreamFormat`` の実装クラスを戻り値に指定します。

    このメソッドは、自動生成される骨格ではすでに宣言されています。

``DataSize getDataSize()``
    入力の推定データサイズを返します。

    省略した場合、データサイズは不明 ( ``DataSize.UNKNOWN`` ) となります。

以下は実装例です。

..  code-block:: java

    public class DocumentFromFile extends DirectFileInputDescription {

        @Override
        public String getBasePath() {
            return "example";
        }

        @Override
        public String getResourcePattern() {
            return "**/data-*.csv";
        }

        @Override
        public Class<?> getModelType() {
            return Document.class;
        }

        @Override
        public Class<? extends BinaryStreamFormat<?>> getFormat() {
            return DocumentCsvFormat.class;
        }

        @Override
        public DataSize getDataSize() {
            return DataSize.LARGE;
        }
    }

..  [#] ``com.asakusafw.vocabulary.directio.DirectFileInputDescription``

入力ファイルのベースパス
~~~~~~~~~~~~~~~~~~~~~~~~
``getBasePath()`` に指定した論理パスは「ベースパス」と呼ばれます。

実行時にはこのベースパスのみを利用して入力元のデータソースを探します。
そのため、以下の2つでは異なる結果になる場合があります。

* ``basePath = "data/asakusa"`` , ``resourcePattern = "file.csv"``
* ``basePath = "data"`` , ``resourcePattern = "asakusa/file.csv"``

上記の場合、 ``data/asakusa`` という論理パスにデータソースが配置されている場合、
それぞれが参照するデータソースは異なるものになります。
この規則について詳しくは、 `論理パスの解決`_ を参照してください。

また、ベースパスには ``${変数名}`` の形式でバッチ引数を利用できます。

入力ファイル名のパターン
~~~~~~~~~~~~~~~~~~~~~~~~
``getResourcePattern()`` にはファイル名だけでなくワイルドカードなどのパターン用の文字列も利用できます。

ここに利用できるパターンは以下の通りです。

..  list-table:: 利用できるパターン
    :widths: 10 10 40
    :header-rows: 1

    * - 文字列
      - 名前
      - 概要
    * - 名前文字
      - リテラル
      - そのままファイル名として利用します。
        対象のデータソースが利用できるファイル名のうち、
        ``/`` , ``\`` , ``$`` , ``*`` , ``?`` , ``#`` , ``|`` , ``{`` , ``}`` , ``[`` , ``]`` 以外の文字を利用できます。
    * - ``/``
      - 名前区切り
      - パスに含まれる名前の区切り文字です。
    * - ``${バッチ引数名}``
      - 変数
      - 実行時にバッチ引数と置き換えます。
        対象のバッチ引数は、変数を含まない任意のパターンの組み合わせである必要があります。
    * - ``*``
      - ワイルドカード
      - 0個以上の任意の名前文字とマッチします。
    * - ``{..|..|..}``
      - 選択
      - ``|`` で区切られたいずれかの名前にマッチします。
        ``..`` の部分には名前文字と名前区切りの組み合わせのみを指定できます。

上記のほかに、特別なディレクトリやファイル名として ``**`` を利用できます。
これは、検索対象以下のすべてのサブディレクトリ(自身のディレクトリも含む)とそれに含まれるファイルにマッチします。

ただし、 ``**`` はディレクトリやファイル名の一部としては利用できません。
たとえば、 ``**.csv`` というパターンは利用できず、代わりに ``**/*.csv`` と書きます。

..  note::
    「変数」に関する挙動は、パターンの解釈の前に一度変数をすべて展開し、
    展開後の文字列をパターンとして解釈して利用しています。

ファイルを出力に利用するDSL
---------------------------
Direct I/Oを利用してファイルからデータを読み出す場合、 ``DirectFileOutputDescription`` [#]_ クラスのサブクラスを作成して必要な情報を記述します。

このクラスでは、下記のメソッドをオーバーライドします。

``String getBasePath()``
    出力に利用する論理パスを戻り値に指定します。

    ここには ``${変数名}`` の形式で、バッチ起動時の引数やあらかじめ宣言された変数を利用できます。
    利用可能な変数はコンテキストAPIで参照できるものと同様です。

``String getResourcePattern()``
    出力に利用するファイル名のパターンを戻り値に指定します。
    ``getBasePath()`` で指定したパスを起点に、このパターンが表すパスにそれぞれのファイルを出力します。

    パターンには ``{property_name:format}`` (プレースホルダ) などを利用できます。
    これは指定したプロパティの内容を、指定のフォーマットでファイル名に埋め込みます。

    詳しくは `出力ファイル名のパターン`_ を参照してください。

``List<String> getOrder()``
    それぞれの出力ファイルの内容をソートするプロパティを指定します。
    
    それぞれのプロパティは ``+property_name`` で昇順、 ``-property_name`` で降順を表します。
    プロパティ名はDMDLのプロパティ名と同様、すべて小文字で単語をアンダースコア ( ``_`` ) で区切ってください。

``Class<?> getModelType()``
    処理対象とするモデルオブジェクトの型を表すクラスを戻り値に指定します。

    このメソッドは、自動生成される骨格ではすでに宣言されています。

``Class<? extends BinaryStreamFormat<?>> getFormat()``
    ``BinaryStreamFormat`` の実装クラスを戻り値に指定します。

    このメソッドは、自動生成される骨格ではすでに宣言されています。

以下は実装例です。

..  code-block:: java

    public class DocumentToFile extends DirectFileOutputDescription {

        @Override
        public String getBasePath() {
            return "example";
        }

        @Override
        public String getResourcePattern() {
            return "{date:yyyy/MM}/data.csv";
        }

        @Override
        public List<String> getOrder() {
            return Arrays.asList("+id");
        }

        @Override
        public Class<?> getModelType() {
            return Document.class;
        }

        @Override
        public Class<? extends BinaryStreamFormat<?>> getFormat() {
            return DocumentCsvFormat.class;
        }
    }

..  [#] ``com.asakusafw.vocabulary.directio.DirectFileOutputDescription``

出力ファイルのベースパス
~~~~~~~~~~~~~~~~~~~~~~~~
``getBasePath()`` に指定した論理パスは「ベースパス」と呼ばれます。

実行時にはこのベースパスのみを利用して出力先のデータソースを探します。
そのため、以下の2つでは異なる結果になる場合があります。

* ``basePath = "data/asakusa"`` , ``resourcePattern = "file.csv"``
* ``basePath = "data"`` , ``resourcePattern = "asakusa/file.csv"``

上記の場合、 ``data/asakusa`` という論理パスにデータソースが配置されている場合、
それぞれが参照するデータソースは異なるものになります。
この規則について詳しくは、 `論理パスの解決`_ を参照してください。

また、ベースパスには ``${変数名}`` の形式でバッチ引数を利用できます。

出力ファイルのベースパスは、次のような制約があります。

* 同一ジョブフローの入力が、ある出力のベースパスと同じまたはそのサブパスであってはならない
* 同一ジョブフローの出力が、ある出力のベースパスと同じまたはそのサブパスであってはならない

..  note::
    上記の制約はトランザクションの制御やテストのために導入した制約です。
    出力に対してはこのような制約がありますが、2つの入力が同じベースパスを利用することは可能です。

出力ファイル名のパターン
~~~~~~~~~~~~~~~~~~~~~~~~
``getResourcePattern()`` にはファイル名だけでなくプロパティの内容からファイル名を計算するための、プレースホルダも利用できます。

ここに利用できるパターンは以下の通りです。

..  list-table:: 出力ファイル名に利用できるパターン
    :widths: 10 10 40
    :header-rows: 1

    * - 文字列
      - 名前
      - 概要
    * - 名前文字
      - リテラル
      - そのままファイル名として利用します。
        対象のデータソースが利用できるファイル名のうち、
        ``/`` , ``\`` , ``$`` , ``*`` , ``?`` , ``#`` , ``|`` , ``{`` , ``}`` , ``[`` , ``]`` 以外の文字を利用できます。
    * - ``/``
      - 名前区切り
      - パスに含まれる名前の区切り文字です。
    * - ``${バッチ引数名}``
      - 変数
      - 実行時にバッチ引数と置き換えます。
        対象のバッチ引数は、名前文字または名前区切りの組み合わせである必要があります。
    * - ``{property:format}``
      - プレースホルダ
      - プロパティの内容を指定のフォーマットで文字列化して利用します。
        プロパティはDMDLと同様に ``snake_case`` の形式でプロパティ名を指定します。

..  warning::
    `入力ファイル名のパターン`_ とは異なり、出力ファイル名のパターンでは変数名にプレースホルダなどを含められません。
    この制約は将来緩和されるかもしれません。

プレースホルダ ( ``{property:format}`` ) には次のようなフォーマットを利用できます。

..  list-table:: プレースホルダに使用できるフォーマット
    :widths: 20 10 60
    :header-rows: 1

    * - 形式
      - データ型
      - 概要
    * - ``:`` とそれ以降を省略
      - すべて
      - ``toString()`` によって文字列化
    * - ``:<日付>``
      - ``DATE``
      - ``:`` 以降を ``SimpleDateFormat.format()`` によって文字列化
    * - ``:<日時>``
      - ``DATETIME``
      - ``:`` 以降を ``SimpleDateFormat.format()`` によって文字列化

``<日付>`` や ``<日時>`` には ``SimpleDateFormat``  [#]_ 形式のパターンを指定します。
たとえば、パターンに ``data/{date:yyyy/MM}.csv`` と指定すると、プロパティ ``date`` の内容を元に ``date/<年>/<月>.csv`` のようなファイルを年と月の情報からそれぞれ作成します。さらに内容をソートするプロパティにも ``date`` を指定すると、ファイルを年と月で分割した後に日にちでソートして出力できます。

出力ファイル名については `出力ファイルの分割と内容のソート`_ も参照してください。

..  attention::
    出力するデータが存在しない場合、ファイルは一つも作成されません。
    これは、ファイル名にプレースホルダを指定していない場合でも同様です。

..  [#] ``java.text.SimpleDateFormat``


アプリケーションのテスト
========================
Direct I/Oを利用したジョブフローやバッチのテストは、Asakusa Frameworkの通常のテスト方法で行えます。
通常のテストについては :doc:`../testing/index` を参照してください。

..  attention::
    現在、ジョブフローの出力に対する初期データの作成 ( ``.prepare()`` ) はサポートしていません。

以下はテスト実行時のテストドライバの挙動です。

入出力のクリア
--------------
テストドライバの入出力が指定された場合、テストの実施前に入出力の対象がすべて削除されます。
このとき、DSLの ``getBasePath()`` で指定した論理パス以下のすべての内容を削除します。

..  warning::
    上記のような挙動のため、データソースの入出力対象はできるだけ制限するようにしてください。

入力データの作成
----------------
入力データの作成時、指定された入力ファイルのパターンに対して一つだけファイルを作成します。
この時、下記のルールをもとに作成するファイルパスを計算します。

..  list-table:: テスト時の入力ファイル名の変換ルール
    :widths: 15 10 40
    :header-rows: 1

    * - 文字列
      - 名前
      - 変換後
    * - 名前文字
      - リテラル
      - そのまま利用します
    * - ``/``
      - 名前区切り
      - そのまま利用します
    * - ``${バッチ引数名}``
      - 変数
      - テストに指定したバッチ引数で置き換えます
    * - ``*``
      - ワイルドカード
      - ``__testing__`` という文字列に置き換えます
    * - ``{..|..|..}``
      - 選択
      - 最左の文字列をそのまま利用します

..  note::
    この規則は暫定的なもので、将来変更されるかもしれません。

出力データの取得
----------------
出力された結果データの取得時、テストドライバはDSLの ``getBasePath()`` で指定した論理パス以下のすべての内容を取得します。
このため、バッチのテストで複数のジョブフローが同一のベースパスに出力を行う場合、正しく動作しません。

..  note::
    この規則は暫定的なもので、将来変更されるかもしれません。


トランザクションのメンテナンス
==============================
Direct I/Oのファイル出力時には、 `簡易的な出力のトランザクション`_ を行っています。
出力を開始する前にシステムディレクトリ [#]_ に対してトランザクションの情報を作成し、
トランザクション処理の完了後にこれらの情報をクリアしています。

以降では、トランザクションが中断された際にこれらを手動で修復する方法について紹介します。
なお、いずれのメンテナンス用コマンドについても、コマンドを起動した環境のHadoopのログ設定 [#]_ を利用してログを出力します。

..  [#] 設定方法については `システムディレクトリの設定`_ を参照してください。
..  [#] 設定方法については `ログの設定`_ を参照してください。

トランザクション情報の一覧を表示
--------------------------------
残っているトランザクション情報の一覧を表示するには、 ``$ASAKUSA_HOME`` 以下の ``directio/bin/list-transaction.sh`` コマンドを引数なしで実行します。
このコマンドを実行すると、以下の情報を表示します。

..  list-table:: 表示されるトランザクションの情報
    :widths: 10 25
    :header-rows: 1

    * - セクション
      - 内容
    * - Date
      - トランザクションを開始した日時
    * - Execution ID
      - 対象のジョブフローの実行ID
    * - Status
      - トランザクションの状態
    * - Comments
      - 補助的な情報

上記のうち、 ``Status`` を調べることで対象のトランザクションの状態が分かります。
特に重要な状態は ``Committed`` (コミット済み) で、この場合には最終的な出力先が不整合な状態になっている場合があります。

また、以降のコマンドでは ``Execution ID`` (実行ID) の情報を元にトランザクションの修復操作を行います。

コミットの適用
--------------
コミット済みのトランザクションを最終的な出力先に反映させるには、 ``$ASAKUSA_HOME`` 以下の ``directio/bin/apply-transaction.sh`` コマンドを実行します。
コマンドの引数にはトランザクションに対応する実行IDを指定してしてください。

このコマンドが対象とするトランザクション処理は、 ``Committed`` (コミット済み) でなければなりません。
それ以外のトランザクション処理に対してこのコマンドを実行しても何も行いません。

このコマンドの実行が成功した場合、トランザクション情報の一覧にコマンドの対象が出現しなくなります。

このコマンドの実行に失敗した場合、出力先のデータソースに何らかの異常が発生している可能性があります。
データソースを正常な状態に戻した後に再度コミットを適用するか、または `トランザクションの破棄`_ を実行して出力に不整合があるままトランザクションを破棄できます。

..  warning::
    コミットを適用する順序には注意が必要です。
    先に適用した出力は、後に適用した出力で上書きされてしまいます。

..  note::
    このコマンドでは、ベストエフォートでのコミットの適用を行っています。
    複数のデータソースが存在し、そのうち一つが常にコミットの適用に失敗してしまう場合、
    即座に適用処理を停止せずにほかのデータソースに対してコミットを適用したのち、エラーとしています。

トランザクションの破棄
----------------------
任意のトランザクション処理を破棄するには、 ``$ASAKUSA_HOME`` 以下の ``directio/bin/abort-transaction.sh`` コマンドを実行します。
コマンドの引数にはトランザクションに対応する実行IDを指定してしてください。

..  warning::
    このコマンドはトランザクションのロールバックを行う **のではなく** 、単にトランザクションを破棄します。
    ``Committed`` (コミット済み) のトランザクションに対してこの処理を実行すると、最終的な出力は不整合な状態になる場合があります。

このコマンドの実行が成功した場合、トランザクション情報の一覧にコマンドの対象が出現しなくなります。
また、それぞれのデータソース上でステージング領域や試行領域の中間データを削除します。

..  attention::
    ただし、ローカルテンポラリ領域 [#]_ 内の試行領域については削除されません。
    これらは別の手段で削除する必要があります。

このコマンドの実行に失敗した場合、出力先のデータソースに何らかの異常が発生している可能性があります。
データソースを正常な状態に戻した後に再度実行するか、またはコマンド実行時のログを参考に、トランザクション情報自体を削除してください。

..  [#] `ローカルテンポラリ領域の設定`_ を参照してください。

